\chapter{Criteria}

From a previous study~\cite{baronyai_publishing_2016} conducted by the author, users at TU Wien are expecting and requesting from a LOD application: \textbf{clear data ownership}, management of \textbf{data freshness} and of \textbf{data quality} and \textbf{maintenance}. Since data ownership is a concern of organisation and cannot be clarified by a tool, data freshness, data quality and maintainability are introduced as criteria. Since data quality is a very generic term, it will be used as criteria category. Additionally a concern of the stakeholders from the paper was how to deal with legacy data, therefore flexibility is included to Data quality.




\begin{itemize}
\item Maintainability
How much effort needs the maintenance? 

\item Data quality 
\begin{itemize}
\item Data freshness (ability to handle new data)
\item Flexibility (of ontology) (deal with heterogenous and/or legacy data)
\end{itemize}

\item Usability: (adopted from Abran et.al.~\cite{abran2003usability} to fit)
\begin{itemize}
\item Effectiveness (How well do the users achieve their goals using the system?)
\item Efficiency (Time to achieve one task, complexity to handle)
\item Satisfaction
\item Security
\item Learnability (Documentation)
\item Performance
\end{itemize}

\item Available data formats (HTML, Relational Databases, Wrapping Existing Application or Web APIs, XML, Tables/Spreadsheets)

%LD book
\item Linked Data Publishing Checklist (from Heath et.al.~\cite{heath2011linked})
\begin{itemize}
\item Does your data set links to other data sets?
\item Do you provide provenance metadata?
\item Do you provide licensing metadata?
\item Do you use terms from widely deployed vocabularies?
\item Are the URIs of proprietary vocabulary terms dereferenceable?
\item Do you map proprietary vocabulary terms to other vocabularies?
\item Do you provide data set-level metadata?
\item Do you refer to additional access methods?
\end{itemize}
\end{itemize}

\section{Rating}

Since a detailed rating scale for each criteria or in each category would go beyond the scope of this paper, a much simpler scale will be used, seen in table~\ref{tbl_rating}. Only the impact will be measured.

\begin{table}[htbp]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Rating} & \textbf{Explanation} \\ \hline
--              & high negative impact \\ \hline
-               & negative impact      \\ \hline
0               & no impact            \\ \hline
+               & positiv impact       \\ \hline
++              & high positiv impact  \\ \hline
\end{tabular}
\caption{Rating scale}
\label{tbl_rating}
\end{table}

\section{Test setting}
To ensure comparable results with different frameworks, the same test setting will be used for each of them. A SQL database is assumed with x \todo{define scale} datasets, representing a publication database. Each framework then will be used to build a SPARQL endpoint under Ubuntu 16.10 (Yakkety Yak) and be rated against the proposed criteria.