\chapter{Methodology (RQ2 \& RQ3)}
\section{About the difficulty of comparing frameworks}~\label{def:framework}

\subsection{The term "framework"}

In order to comparing frameworks of the field of Linked Data, a first step must be to define \emph{what} a framework actually is, since it is a very generic term. One way to define it could be the definition by Roberts and Johnson,~\cite{roberts1996evolving}:

\begin{quotation}
Frameworks are reusable designs of all part of a software described by a set of abstract classes and the way instances of those collaborate
\end{quotation}

Another way could be the explanation by Riehle in his PhD thesis,~\cite{riehle2000framework}:

\begin{quotation}
Frameworks model a specific domain or an important aspect thereof. They represent the domain as an abstract design, consisting of abstract classes (or interfaces). The abstract design is more than a set of classes, because it defines how instances of the classes are allowed to collaborate with each other at runtime. Effectively, it acts as a skeleton, or a scaffolding, that determines how framework objects relate to each other.

A framework comes with reusable implementations in the form of abstract and concrete class implementations. Abstract implementations are abstract classes that implement parts of a framework abstraction (as expressed by an abstract class or interface), but leave crucial implementation decisions to subclasses. [...]
\end{quotation}

Both of them refer frameworks as tools for coding, used when writing own applications. One of the most classical examples might be the Spring Framework in the Java world.

But the problem is here, that the term is not always used and understand in this way, LDIF and the Silk frameworks define themselves as such, but providing in facts a set of tools without necessarily needing coding to work with them (except configuration files). Others may see tools like the Information Workbench or D2RQ as a framework for publishing.

On a higher level, the architectures proposed in section~\ref{arch_frameworks} might be seen as a high-level or meta-framework. And since the proposed tools in the other sections (partially) are using these architectures, one could argue, that they therefore are also frameworks.

\subsection{Defining the limits}

Next to the general problem about the term "framework", another problem is to set the borders of the examine topic. Since the paper aims to compare "Linked Data frameworks", the goal is to cover the whole process of publishing Linked Data, from the bottom persistence layer of accessing existing data, transforming data formats (e.g. relational to RDF), over cleaning and interlinking the data, over storing them in a triple store, up to making them available over an interface like SPARQL.

But there are not many tools/frameworks covering the whole process and supporting different data formats (e.g. relational data and CSV) at the same time. There are some tools like D2RQ only focusing on specific data formats, but providing the full stack, some tools like LDIF only focusing on a specific part of the process, without e.g. providing capabilities for SPARQL endpoints.

The best way is maybe using a stack of different tools to cover the whole workflow, combining them like Silk is integrated in LDIF. Or using the generic architecture, coding an own application and using partially the proposed tools.

But covering different areas, it is difficult to actually compare them. How to compare a persistence framework with a GUI framework?

\section{Criteria}

\begin{itemize}
\item Maintainability
How much effort needs the maintenance? 

\item Data quality 
\begin{itemize}
\item Data freshness (ability to handle new data)
\item Flexibility (of ontology) (deal with heterogenous and/or legacy data)
\end{itemize}

\item Usability: (adopted from Abran et.al.~\cite{abran2003usability} to fit)
\begin{itemize}
\item Effectiveness (How well do the users achieve their goals using the system?)
\item Efficiency (Time to achieve one task, complexity to handle)
\item Satisfaction
\item Security
\item Learnability (Documentation)
\item Performance
\end{itemize}

\item Available data formats (HTML, Relational Databases, Wrapping Existing Application or Web APIs, XML, Tables/Spreadsheets)

%LD book
\item Linked Data Publishing Checklist (from Heath et.al.~\cite{heath2011linked})
\begin{itemize}
\item Does your data set links to other data sets?
\item Do you provide provenance metadata?
\item Do you provide licensing metadata?
\item Do you use terms from widely deployed vocabularies?
\item Are the URIs of proprietary vocabulary terms dereferenceable?
\item Do you map proprietary vocabulary terms to other vocabularies?
\item Do you provide data set-level metadata?
\item Do you refer to additional access methods?
\end{itemize}
\end{itemize}

\subsection{Rating}

Since a detailed rating scale for each criteria or in each category would go beyond the scope of this paper, a much simpler scale will be used, seen in table~\ref{tbl_rating}. Only the impact will be measured.

\begin{table}[htbp]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Rating} & \textbf{Explanation} \\ \hline
--              & high negative impact \\ \hline
-               & negative impact      \\ \hline
0               & no impact            \\ \hline
+               & positiv impact       \\ \hline
++              & high positiv impact  \\ \hline
\end{tabular}
\caption{Rating scale}
\label{tbl_rating}
\end{table}

\section{Test setting}
To ensure comparable results with different frameworks, the same test setting will be used for each of them. A SQL database is assumed with x \todo{define scale} datasets, representing a publication database. Each framework then will be used to build a SPARQL endpoint under Ubuntu 16.10 (Yakkety Yak) and be rated against the proposed criteria.